---
title: "STT_180_College_Football_Analysis"
author: "Mark Endicott, Jenna Coyle, Sarah Bejleri, Raunak Chattopadhyay"
Started: "2024-11-08"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(caret)
library(dplyr)
library(ggplot2)
library(broom)

```

## Questions We Must Answer

(1) What features (variables) of a football team translate into wins?

(2) Which Collegiate Conference is the “best”?

(3) Does defense really wins championships?

# Addressing Question 1 (Mark)

From Kaggle, we extracted 11 different data sets with similar variables. To find the features most predictive of "wins," we first acknowledged variables containing the "wins" key-word as our responses. First, we tried just 2013 For this data frame, we filtered and selected numerical variables, corrected for linear dependence, and removed missing values. Once the data was ready, we performed backwards elimination to produce a parsimonious model.

```{r modeling fb_data13}

# Step 1: Read in the data
fb_data <- read.csv("cfb13.csv", header = TRUE)

# Step 2: Select only numeric variables
fb_data13 <- fb_data %>% select_if(is.numeric)

# Step 3: Remove highly correlated variables
# Calculate correlation matrix
cor_matrix <- cor(fb_data13)

# Set a threshold for high correlation (e.g., 0.8)
cor_threshold <- 0.8

# Find highly correlated pairs
high_corr_pairs <- findCorrelation(cor_matrix, cutoff = cor_threshold)

# Remove highly correlated variables
fb_data13 <- fb_data13[, -high_corr_pairs]

# Add the target variable 'Wins' to the dataset
fb_data13$Wins <- fb_data$Win

# Step 4: Remove variables with zero variance
fb_data13 <- fb_data13[, sapply(fb_data13, function(x) var(x) != 0)]

# Step 5: Remove rows with missing values
fb_data13 <- na.omit(fb_data13)

# Step 6: Fit the initial linear model with all variables
initial_model <- lm(Wins ~ ., data = fb_data13)

# Step 7: Perform backward elimination
final_model <- step(initial_model, direction = "backward", trace = 0)

summary(final_model)

```

After accounting for non-numerical values, missing values, and multicollinearity, we have a model that represents approximately 96% of the data. Additionally, we can still make this model more parsimonious by eliminating variables with high p-values.

```{r manual_parsimony}

# USED GPT TO MANUALLY SELECT VARIABLES WHOSE P-VALUE IS GREATER THAN .05.

# Fit the reduced model by removing variables with p-value > 0.05
reduced_model <- lm(Wins ~ Games + Opp.First.Down.Runs + X4th.Percent + X4rd.Down.Def.Rank +
                    Pass.Attempts + Pass.Yards.Attempt + Yards.Completion + 
                    Penalty.Rank + Punt.Return.Def.Rank + Opp.Punt.Return.Touchdowns.Allowed + 
                    Redzone.Off.Rank + Redzone.Rush.TD + Redzone.Pass.TD + Redzone.Field.Goals.Made + 
                    Redzone.Def.Rank + Sacks + Opp.Deflected.Extra.Points + 
                    Turnovers.Gain + Interceptions.Thrown.y, data = fb_data13)

# View summary of the reduced model
summary(reduced_model)

# AGAIN

# Fit the reduced model by removing variables with p-value > 0.05
reduced_model13 <- lm(Wins ~ Games + Opp.First.Down.Runs + X4rd.Down.Def.Rank + 
                    Pass.Attempts + Pass.Yards.Attempt + Yards.Completion + 
                    Punt.Return.Def.Rank + Redzone.Rush.TD + Redzone.Pass.TD + 
                    Redzone.Field.Goals.Made + Sacks + Turnovers.Gain + 
                    Interceptions.Thrown.y, data = fb_data13)

# View summary of the reduced model
summary(reduced_model13)

```

Now our model looks more parsimonious. Now we can follow this exact process for 2023 and compare the models. Note: Several adjustments were made for the 2023 data set.

```{r modeling fb_data23}

# Step 1: Read in the data
fb_data <- read.csv("cfb23.csv", header = TRUE)

# Step 2: Convert all compatible columns to numeric type and include relavent variables

fb_data <- fb_data %>%
  separate(Win.Loss, into = c("Wins", "Losses"), sep = "-") %>%
  mutate(Wins = as.numeric(Wins)) # Extracting Wins

Wins <- fb_data$Wins

fb_data23 <- fb_data %>%
  mutate_all(~ as.numeric(as.character(.)))

fb_data23 <- fb_data23 %>% 
  select(-c(Team, Time.of.Possession, Average.Time.of.Possession.per.Game))

# Step 3: Remove rows with missing values
fb_data23 <- na.omit(fb_data23)

# Step 4: Remove highly correlated variables
# Calculate correlation matrix
cor_matrix <- cor(fb_data23)

# Set a threshold for high correlation (e.g., 0.8)
cor_threshold <- 0.8

# Find highly correlated pairs
high_corr_pairs <- findCorrelation(cor_matrix, cutoff = cor_threshold)

# Remove highly correlated variables
fb_data23 <- fb_data23[, -high_corr_pairs]

# Step 5: Remove variables with zero variance
fb_data23 <- fb_data23[, sapply(fb_data23, function(x) var(x) != 0)]

# Step 6: Reinsert Wins
fb_data23$Wins <- Wins[1:130]
fb_data23 <- na.omit(fb_data23)

# Step 7: Fit the initial linear model with all variables
initial_model <- lm(Wins ~ ., data = fb_data23)

# Step 8: Perform backward elimination
final_model <- step(initial_model, direction = "backward", trace = 0)

summary(final_model)

```

Now we make the data parsimonious.

```{r}

# AGAIN, MANUALLY DONE WITH CHAT GPT.

# Updated linear model excluding variables with high p-values
model_updated <- lm(formula = Wins ~ Games + X3rd.Attempts + X3rd.Conversions + 
                            X3rd.Down.Def.Rank + Opp.3rd.Conversion + Opp.3rd.Attempt + 
                            X4th.Attempts + X4th.Conversions + Opp.4th.Conversion + Opp.4th.Attempt + 
                            First.Down.Rank + First.Down.Passes + 
                            Yards.Completion + Punt.Returns + Opp.Rush.Attempts + 
                            Turnovers.Gain + Fumbles.Lost + Interceptions.Thrown_y, 
                    data = fb_data23)

# View summary of the updated model
summary(model_updated)


```

Now lets do the same thing for a middle year, 2018!

```{r}

# Step 1: Read in the data
fb_data <- read.csv("cfb18.csv", header = TRUE)

# Step 2: Select only numeric variables
fb_data18 <- fb_data %>% select_if(is.numeric)

# Step 3: Remove highly correlated variables
# Calculate correlation matrix
cor_matrix <- cor(fb_data18)

# Set a threshold for high correlation (e.g., 0.8)
cor_threshold <- 0.8

# Find highly correlated pairs
high_corr_pairs <- findCorrelation(cor_matrix, cutoff = cor_threshold)

# Remove highly correlated variables
fb_data18 <- fb_data18[, -high_corr_pairs]

# Add the target variable 'Wins' to the dataset
fb_data18$Wins <- fb_data$Win

# Step 4: Remove variables with zero variance
fb_data18 <- fb_data18[, sapply(fb_data18, function(x) var(x) != 0)]

# Step 5: Remove rows with missing values
fb_data18 <- na.omit(fb_data18)

# Step 6: Fit the initial linear model with all variables
initial_model <- lm(Wins ~ ., data = fb_data18)

# Step 7: Perform backward elimination
final_model <- step(initial_model, direction = "backward", trace = 0)

summary(final_model)

```

```{r }

# Linear model with only significant predictors
model <- lm(Wins ~ Games + Loss + Opp.Punt.Returns + 
            Opp.Net.Punt.Return.Yards + Avg.Yards.Allowed.per.Punt.Return + 
            Sack.Yards + Opp.Redzone.Pass.Touchdowns.Allowed + 
            Opp.Deflected.Extra.Points, data = fb_data18)

# View the model summary
summary(model)

```
Unfortunately, this turned out to be a perfect fit, which signals a less reliable model.

## Result

93-95% fit and all the variables are statistically significant. The number of games is highly predictive of wins. Most other variable are negligible for these two years.

# Addressing Question 2 (Jenna & Sarah)


```{r}
# Reading in our Data
cfb13 = read.csv("cfb13.csv", header = TRUE, sep = ",")
cfb14 = read.csv("cfb14.csv", header = TRUE, sep = ",")
cfb15 = read.csv("cfb15.csv", header = TRUE, sep = ",")
cfb16 = read.csv("cfb16.csv", header = TRUE, sep = ",")
cfb17 = read.csv("cfb17.csv", header = TRUE, sep = ",")
cfb18 = read.csv("cfb18.csv", header = TRUE, sep = ",")
cfb19 = read.csv("cfb19.csv", header = TRUE, sep = ",")
cfb20 = read.csv("cfb20.csv", header = TRUE, sep = ",")
```


```{r}
# Loading in Libraries
library(tidyverse)
library(broom)
library(dplyr)
library(ggplot2)
```



```{r}
# Calculating Win Percentage
cfb13$Win_Percentage = (cfb13$Win / cfb13$Games) * 100
cfb14$Win_Percentage = (cfb14$Win / cfb14$Games) * 100
cfb15$Win_Percentage = (cfb15$Win / cfb15$Games) * 100
cfb16$Win_Percentage = (cfb16$Win / cfb16$Games) * 100
cfb17$Win_Percentage = (cfb17$Win / cfb17$Games) * 100
cfb18$Win_Percentage = (cfb18$Win / cfb18$Games) * 100
cfb19$Win_Percentage = (cfb19$Win / cfb19$Games) * 100
cfb20$Win_Percentage = (cfb20$Win / cfb20$Games) * 100
```

```{r}
# Separates Conference into new column (original datasets had them with team name 
# in parenthesis, hence the values inside gsub)
cfb13$Conference = gsub(".*\\(([^)]+)\\)", "\\1", cfb13$Team)
cfb13$Team = gsub(" \\([^)]*\\)", "", cfb13$Team)

cfb14$Conference = gsub(".*\\(([^)]+)\\)", "\\1", cfb14$Team)
cfb14$Team = gsub(" \\([^)]*\\)", "", cfb14$Team)

cfb15$Conference = gsub(".*\\(([^)]+)\\)", "\\1", cfb15$Team)
cfb15$Team = gsub(" \\([^)]*\\)", "", cfb15$Team)

cfb16$Conference = gsub(".*\\(([^)]+)\\)", "\\1", cfb16$Team)
cfb16$Team = gsub(" \\([^)]*\\)", "", cfb16$Team)

cfb17$Conference = gsub(".*\\(([^)]+)\\)", "\\1", cfb17$Team)
cfb17$Team = gsub(" \\([^)]*\\)", "", cfb17$Team)

cfb18$Conference = gsub(".*\\(([^)]+)\\)", "\\1", cfb18$Team)
cfb18$Team = gsub(" \\([^)]*\\)", "", cfb18$Team)

cfb19$Conference = gsub(".*\\(([^)]+)\\)", "\\1", cfb19$Team)
cfb19$Team = gsub(" \\([^)]*\\)", "", cfb19$Team)

cfb20$Conference = gsub(".*\\(([^)]+)\\)", "\\1", cfb20$Team)
cfb20$Team = gsub(" \\([^)]*\\)", "", cfb20$Team)
```



```{r}
# Adding all datasets to a list that can then be looped through
cfb_list = list(cfb13, cfb14, cfb15, cfb16, cfb17, cfb18, cfb19, cfb20)

# Creates model for each year of data
analyze_conference = function(data) {
  model = lm(Win_Percentage ~ Conference + Off.Rank + Def.Rank + Points.Per.Game + Avg.Points.per.Game.Allowed, data = data)
  summary(model)
}

results = lapply(cfb_list, analyze_conference)

names(results) <- paste0("cfb", 13:20)
results
```


```{r}
# Cleaning the data to ensure all datasets have the same column names and order
standardize_columns = function(data) {
  # Define the required column names
  required_cols = c("Conference", "Win_Percentage", "Off.Rank", 
                     "Def.Rank", "Points.Per.Game", "Avg.Points.per.Game.Allowed")
  # Add missing columns with NA values
  missing_cols = setdiff(required_cols, colnames(data))
  for (col in missing_cols) {
    data[[col]] = NA}
  # Reorder columns
  data = data[, required_cols, drop = FALSE]
  return(data)}

# Apply the function to all cfb datasets
cfb_list_standardized = lapply(cfb_list, standardize_columns)

# Combine cleaned datasets
cfb_combined = do.call(rbind, cfb_list_standardized)

# Analyze each year using the function
analyze_conference = function(data) {
  model = lm(Win_Percentage ~ Conference + Off.Rank + Def.Rank + 
                Points.Per.Game + Avg.Points.per.Game.Allowed, data = data)
  summary(model)
}

# Apply the function to each year's data
results = lapply(cfb_list, analyze_conference)

# Combine results into one summary for analyzing
combined_summary = lapply(results, function(res) {
  data.frame(Coefficient = rownames(res$coefficients), 
             Estimate = res$coefficients[, "Estimate"], 
             P_Value = res$coefficients[, "Pr(>|t|)"])
})
```

```{r}
# Find IQR for each conference
conference_iqr = cfb_combined %>%
  group_by(Conference) %>%
  summarize(IQR = IQR(Win_Percentage, na.rm = TRUE)) %>%
  arrange(desc(IQR))

# Select conferences with the highest IQR
top_iqr_conferences = conference_iqr %>%
  slice_head(n = 12) %>%
  pull(Conference)

cfb_filtered = cfb_combined %>%
  filter(Conference %in% top_iqr_conferences)

# ^ This part of the code eliminates an error within the datasets, some schools were not filed into the data correctly


# 1. Win Percentage by Conference (combined years)
ggplot(cfb_filtered, aes(x = Conference, y = Win_Percentage, fill = Conference)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Win Percentage by Conferences (Combined Years)", 
       y = "Win Percentage", x = "Conference") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


# 2. Points Per Game vs. Win Percentage (combined years)
ggplot(cfb_filtered, aes(x = Points.Per.Game, y = Win_Percentage)) +
  geom_point(aes(color = Conference), size = 2, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "black", linetype = "dashed") +
  theme_minimal() +
  labs(title = "Points Per Game vs. Win Percentage (Combined Years)", 
       x = "Points Per Game", y = "Win Percentage")

# 3. Average Points Per Game Allowed vs. Win Percentage (combined years)
ggplot(cfb_filtered, aes(x = Avg.Points.per.Game.Allowed, y = Win_Percentage)) +
  geom_point(aes(color = Conference), size = 2, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "black", linetype = "dashed") +
  theme_minimal() +
  labs(title = "Avg. Points Per Game Allowed vs. Win Percentage (Combined Years)", 
       x = "Avg. Points Per Game Allowed", y = "Win Percentage")

# 4. Offense vs. Defense Rank (combined years)
ggplot(cfb_filtered, aes(x = Off.Rank, y = Def.Rank, color = Win_Percentage)) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_gradient(low = "blue", high = "red") +
  theme_minimal() +
  labs(title = "Offense vs. Defense Rank (Combined Years)", 
       x = "Offensive Rank", y = "Defensive Rank", color = "Win %")

# 5. Summary statistics by Conference (combined years)
conference_summary_combined = cfb_filtered %>%
  group_by(Conference) %>%
  summarise(Avg_Win_Percentage = mean(Win_Percentage, na.rm = TRUE),
            Avg_Off_Rank = mean(Off.Rank, na.rm = TRUE),
            Avg_Def_Rank = mean(Def.Rank, na.rm = TRUE),
            Avg_Points_Per_Game = mean(Points.Per.Game, na.rm = TRUE),
            Avg_Allowed = mean(Avg.Points.per.Game.Allowed, na.rm = TRUE))

print(conference_summary_combined)
```


Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

# Addressing Question 3 (Raunak)

```{r libraries & loading in data}
library(tidyverse)
library(ggplot2)
library(broom)

cfb13 <- read.csv("cfb13.csv")
cfb14 <- read.csv("cfb14.csv")
cfb15 <- read.csv("cfb15.csv")
cfb16 <- read.csv("cfb16.csv")
cfb17 <- read.csv("cfb17.csv")
cfb18 <- read.csv("cfb18.csv")
cfb19 <- read.csv("cfb19.csv")
cfb20 <- read.csv("cfb20.csv")
```

(1) Initial Data Analysis
    - Choosing three years of data to compare and contrast results, in this case 2013, 2018, and 2023
    - Once the three years are chosen calculate the win percentage for each year by mutating the dataset
    - Plot the win_percentage vs defensive ranking to get a basic understand of whether defense impacts championships
```{r initial analysis}
# Create win percentage column and add it to the data set
cfb13$win_pct <- cfb13$Win/cfb13$Games
cfb14$win_pct <- cfb14$Win/cfb14$Games
cfb15$win_pct <- cfb15$Win/cfb15$Games
cfb16$win_pct <- cfb16$Win/cfb16$Games
cfb17$win_pct <- cfb17$Win/cfb17$Games
cfb18$win_pct <- cfb18$Win/cfb18$Games
cfb19$win_pct <- cfb19$Win/cfb19$Games
cfb20$win_pct <- cfb20$Win/cfb20$Games

# Plotting Def.Rank and Win Percentage for each year (2013 to 2020) and overlaying it with a fitted linear model for each year
ggplot() +
  geom_point(data = cfb13, aes(x = Def.Rank, y = win_pct, color = "2013")) +
  geom_smooth(data = cfb13, aes(x = Def.Rank, y = win_pct, color = "2013"), method = "lm", se = FALSE) +
  labs(x = "Defensive Rank",
       y = "Win Percentage",
       title = "Defense Rank vs Win Percentage in 2013",
       color = "Year")


ggplot() + 
  geom_point(data = cfb14, aes(x = Def.Rank, y = win_pct, color = "2014")) +
  geom_smooth(data = cfb14, aes(x = Def.Rank, y = win_pct, color = "2014"), method = "lm", se = FALSE) +
  labs(x = "Defensive Rank",
       y = "Win Percentage",
       title = "Defense Rank vs Win Percentage in 2014",
       color = "Year")


ggplot() +
  geom_point(data = cfb15, aes(x = Def.Rank, y = win_pct, color = "2015")) +
  geom_smooth(data = cfb15, aes(x = Def.Rank, y = win_pct, color = "2015"), method = "lm", se = FALSE) +
  labs(x = "Defensive Rank",
       y = "Win Percentage",
       title = "Defense Rank vs Win Percentage in 2015",
       color = "Year")


ggplot() +
  geom_point(data = cfb16, aes(x = Def.Rank, y = win_pct, color = "2016")) +
  geom_smooth(data = cfb16, aes(x = Def.Rank, y = win_pct, color = "2016"), method = "lm", se = FALSE) + 
  labs(x = "Defensive Rank",
       y = "Win Percentage",
       title = "Defense Rank vs Win Percentage in 2016",
       color = "Year")


ggplot() +
  geom_point(data = cfb17, aes(x = Def.Rank, y = win_pct, color = "2017")) +
  geom_smooth(data = cfb17, aes(x = Def.Rank, y = win_pct, color = "2017"), method = "lm", se = FALSE) + 
  labs(x = "Defensive Rank",
       y = "Win Percentage",
       title = "Defense Rank vs Win Percentage in 2017",
       color = "Year")


ggplot() +
  geom_point(data = cfb18, aes(x = Def.Rank, y = win_pct, color = "2018")) +
  geom_smooth(data = cfb18, aes(x = Def.Rank, y = win_pct, color = "2018"), method = "lm", se = FALSE) + 
  labs(x = "Defensive Rank",
       y = "Win Percentage",
       title = "Defense Rank vs Win Percentage in 2018",
       color = "Year")


ggplot() +
  geom_point(data = cfb19, aes(x = Def.Rank, y = win_pct, color = "2019")) +
  geom_smooth(data = cfb19, aes(x = Def.Rank, y = win_pct, color = "2019"), method = "lm", se = FALSE) + 
  labs(x = "Defensive Rank",
       y = "Win Percentage",
       title = "Defense Rank vs Win Percentage in 2019",
       color = "Year")



ggplot() +
  geom_point(data = cfb20, aes(x = Def.Rank, y = win_pct, color = "2020")) +
  geom_smooth(data = cfb20, aes(x = Def.Rank, y = win_pct, color = "2020"), method = "lm", se = FALSE) + 
  labs(x = "Defensive Rank",
       y = "Win Percentage",
       title = "Defense Rank vs Win Percentage in 2020",
       color = "Year")
```

(2) Individual year analysis
    - Create function to neatly output linear regression data for each year
    - Input our three years
```{r year analysis}
# created a regression model for each year (2013 - 2020)
regression13 <- lm(win_pct ~ Def.Rank, data = cfb13)
regression14 <- lm(win_pct ~ Def.Rank, data = cfb14)
regression15 <- lm(win_pct ~ Def.Rank, data = cfb15)
regression16 <- lm(win_pct ~ Def.Rank, data = cfb16)
regression17 <- lm(win_pct ~ Def.Rank, data = cfb17)
regression18 <- lm(win_pct ~ Def.Rank, data = cfb18)
regression19 <- lm(win_pct ~ Def.Rank, data = cfb19)
regression20 <- lm(win_pct ~ Def.Rank, data = cfb20)

# output regression in a neat way using tidy
tidy(regression13)
tidy(regression14)
tidy(regression15)
tidy(regression16)
tidy(regression17)
tidy(regression18)
tidy(regression19)
tidy(regression20)
```

(3) Comparing Offense and Defense Correlation to Wins
    - Created a single fitted model for offense and win rate to compare with the single fitted model consisting of defensive rating and win percentage

``` {r comparison}
# Creating data frame which compares r^2 values from each fitted model
def13 <- lm(win_pct ~ Def.Rank, data = cfb13)
off13 <- lm(win_pct ~ Off.Rank, data = cfb13)
r2_2013 <- data.frame(
  year = 2013,
  defense_r2 = summary(def13)$r.squared,
  offense_r2 = summary(off13)$r.squared
)


def14 <- lm(win_pct ~ Def.Rank, data = cfb14)
off14 <- lm(win_pct ~ Off.Rank, data = cfb14)
r2_2014 <- data.frame(
  year = 2014,
  defense_r2 = summary(def14)$r.squared,
  offense_r2 = summary(off14)$r.squared
)


def15 <- lm(win_pct ~ Def.Rank, data = cfb15)
off15 <- lm(win_pct ~ Off.Rank, data = cfb15)
r2_2015 <- data.frame(
  year = 2015,
  defense_r2 = summary(def15)$r.squared,
  offense_r2 = summary(off15)$r.squared
)


def16 <- lm(win_pct ~ Def.Rank, data = cfb16)
off16 <- lm(win_pct ~ Off.Rank, data = cfb16)
r2_2016 <- data.frame(
  year = 2016,
  defense_r2 = summary(def16)$r.squared,
  offense_r2 = summary(off16)$r.squared
)


def17 <- lm(win_pct ~ Def.Rank, data = cfb17)
off17 <- lm(win_pct ~ Off.Rank, data = cfb17)
r2_2017 <- data.frame(
  year = 2017,
  defense_r2 = summary(def17)$r.squared,
  offense_r2 = summary(off17)$r.squared
)


def18 <- lm(win_pct ~ Def.Rank, data = cfb18)
off18 <- lm(win_pct ~ Off.Rank, data = cfb18)
r2_2018 <- data.frame(
  year = 2018,
  defense_r2 = summary(def18)$r.squared,
  offense_r2 = summary(off18)$r.squared
)


def19 <- lm(win_pct ~ Def.Rank, data = cfb19)
off19 <- lm(win_pct ~ Off.Rank, data = cfb19)
r2_2019 <- data.frame(
  year = 2019,
  defense_r2 = summary(def19)$r.squared,
  offense_r2 = summary(off19)$r.squared
)


def20 <- lm(win_pct ~ Def.Rank, data = cfb20)
off20 <- lm(win_pct ~ Off.Rank, data = cfb20)
r2_2020 <- data.frame(
  year = 2020,
  defense_r2 = summary(def20)$r.squared,
  offense_r2 = summary(off20)$r.squared
)

# Combine results (had to consult stack overflow to use rbind to combine results)
all_comparisons <- rbind(r2_2013, r2_2014, r2_2015, r2_2016, r2_2017, r2_2018, r2_2019, r2_2020)
print(all_comparisons)
```


